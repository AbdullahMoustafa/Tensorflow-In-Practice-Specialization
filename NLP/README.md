# Natural language processing tutorials.
1st | 2nd | 3rd week
. Text classification as BBC news and sercastic.

4th week
. predict the most common word that comes next in the sequence
. create new sets of words using their embeddings!

First week - Explore the BBC news archive
For this exercise get the BBC text archive.
1- tokenize the dataset,
2- removing common stopwords.
Passed 100%

Second week - BBC news archive
This week you will build on last weekâ€™s exercise where you tokenized words from the BBC news reports dataset. This dataset contains articles that are classified into a number of different categories. See if you can design a neural network that can be trained on this dataset to accurately determine what words determine what category. Create the vecs.tsv and meta.tsv files and load them into 
the embedding projector:  https://projector.tensorflow.org/

